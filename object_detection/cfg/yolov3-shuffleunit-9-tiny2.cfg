[net]
# Testing
#batch=1
#subdivisions=1
# Training
batch=16
subdivisions=1
width=416
height=416
channels=3
momentum=0.9
decay=0.0005
angle=0
saturation = 1.5
exposure = 1.5
hue=.1

learning_rate=0.001
burn_in=1000
max_batches = 500200
policy=steps
steps=400000,450000
scales=.1,.1

[convolutional]
batch_normalize=1
filters=24
size=3
stride=1
pad=1
activation=leaky

# Downsample

[convolutional]
batch_normalize=1
filters=32
size=3
stride=2
pad=1
activation=leaky

# 1x        3
[shuffleblock]
filters=32


[shuffleblock]
filters=32

# Downsample     5

[shuffleblock]
stride = 2
filters=64

# 2x 1
[shuffleblock]
filters=64


[shuffleblock]
filters=128

# 2x 2
[shuffleblock]
filters=128


[shuffleblock]
filters=128

# Downsample    10
[shuffleblock]
stride = 2
filters=256

# 1 8x1
[shuffleblock]
filters=256


# 1 8x 2
[shuffleblock]
filters=256


# 1 8x 3        13
[shuffleblock]
filters=256


# 1 8x 4
[shuffleblock]
filters=256


# 1 8x 5       15
[shuffleblock]
filters=256

# 1 8x 6
[shuffleblock]
filters=256


# Downsample
[shuffleblock]
stride = 2
filters=512


# 2 8x 1
[shuffleblock]
filters=512

# 2 8x 2     19
[shuffleblock]
filters=512


# 2 8x 3
[shuffleblock]
filters=512


# 2 8x 4      21
[shuffleblock]
filters=512


# 2 8x 5
[shuffleblock]
filters=512

# 2 8x 6
[shuffleblock]
filters=512




# Downsample      24
# 4x   0
[shuffleblock]
stride = 2
filters=1024



#4x   1
[shuffleblock]
filters=1024



#4x   2
[shuffleblock]
filters=1024



# 4x3
[shuffleblock]
filters=1024



# 4x   4        28
[shuffleblock]
filters=1024




[convolutional]
batch_normalize=1
filters=512
size=1
stride=1
pad=1
activation=leaky
group=2

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky
group=2

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

# 32
[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky

[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=96
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=21
activation=linear

# 36
[yolo]
mask = 6,7,8
anchors = 12,24, 18,32, 28,66, 38,129, 40,70, 47,218, 67,124, 81,250, 150,313
classes=2
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1


[route]
layers = -4

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
group=2

[upsample]
stride=2

# 40
[route]
layers = -1, 22



[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky
group=2

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
filters=96
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=64
activation=leaky

[convolutional]
size=1
stride=1
pad=1
filters=21
activation=linear


[yolo]
mask = 3,4,5
anchors = 12,24, 18,32, 28,66, 38,129, 40,70, 47,218, 67,124, 81,250, 150,313
classes=2
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1



[route]
layers = -4

[convolutional]
batch_normalize=1
filters=256
size=1
stride=1
pad=1
activation=leaky
group = 2


[upsample]
stride=2

[route]
layers = -1, 15



[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=256
activation=leaky
group=2

[convolutional]
batch_normalize=1
filters=128
size=1
stride=1
pad=1
activation=leaky


[convolutional]
batch_normalize=1
size=3
stride=1
pad=1
filters=96
activation=leaky


[convolutional]
batch_normalize=1
filters=64
size=1
stride=1
pad=1
activation=leaky


[convolutional]
size=1
stride=1
pad=1
filters=21
activation=linear


[yolo]
mask = 0,1,2
anchors = 12,24, 18,32, 28,66, 38,129, 40,70, 47,218, 67,124, 81,250, 150,313
classes=2
num=9
jitter=.3
ignore_thresh = .7
truth_thresh = 1
random=1
